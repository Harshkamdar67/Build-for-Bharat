{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StxjK78Nrqm1",
        "outputId": "9e9019dc-cc01-4dc7-a6b8-b628354a98f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.26.1\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.26.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ew8KXoTfYKY",
        "outputId": "2fdcb9cb-a482-4153-80c4-f66d66a29f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U ctransformers[cuda]\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers scipy ftfy accelerate\n",
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1.2 --progress-bar off\n",
        "!pip install -qqq transformers==4.36.2 --progress-bar off\n",
        "!pip install -qqq einops==0.7.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.25.0 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfp2Y9O6xnH-"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzyWddg0mwDD",
        "outputId": "70f22b20-0d00-46fa-9c99-9d36cdc46087"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWXJRaYnnGWi"
      },
      "outputs": [],
      "source": [
        "sentences = [\"Your negotiation text here.\", \"Another sentence.\"]\n",
        "tokenized_texts = [tokenizer.tokenize(sentence) for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p1-GMCsnOxi",
        "outputId": "03b8c158-f76a-43b6-bbf5-f5f88b1153d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['your', 'negotiation', 'text', 'here', '.'], ['another', 'sentence', '.']]"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOLSxEMJtK8s"
      },
      "outputs": [],
      "source": [
        "training_data = {\n",
        "    \"The iPhone 13 Pro is priced at $999, but we have a limited-time offer with a 20% discount.\": {\n",
        "        \"Product\": \"iPhone 13 Pro\",\n",
        "        \"Price\": \"$999\",\n",
        "        \"Discount\": \"20%\"\n",
        "    },\n",
        "    \"The Galaxy Watch 4 comes with a 2-year warranty and free shipping.\": {\n",
        "        \"Product\": \"Galaxy Watch 4\",\n",
        "        \"Warranty\": \"2 years\",\n",
        "        \"Shipping\": \"Free\"\n",
        "    },\n",
        "    \"Our new smart TV model features a 55-inch 4K OLED display and built-in streaming apps.\": {\n",
        "        \"Product\": \"Smart TV\",\n",
        "        \"Display\": \"55-inch 4K OLED\",\n",
        "        \"Features\": \"Built-in streaming apps\"\n",
        "    },\n",
        "    \"This laptop has an Intel Core i7 processor, 16GB RAM, and a 512GB SSD for fast performance.\": {\n",
        "        \"Product\": \"Laptop\",\n",
        "        \"Processor\": \"Intel Core i7\",\n",
        "        \"RAM\": \"16GB\",\n",
        "        \"Storage\": \"512GB SSD\"\n",
        "    },\n",
        "    \"Our vacation package includes a 7-day stay at a 5-star beachfront resort and daily breakfast.\": {\n",
        "        \"Package\": \"Vacation package\",\n",
        "        \"Duration\": \"7 days\",\n",
        "        \"Accommodation\": \"5-star beachfront resort\",\n",
        "        \"Inclusions\": \"Daily breakfast\"\n",
        "    },\n",
        "    \"The annual subscription to our streaming service offers unlimited access to a vast library of movies and TV shows.\": {\n",
        "        \"Service\": \"Streaming service\",\n",
        "        \"Subscription\": \"Annual\",\n",
        "        \"Content\": \"Movies and TV shows\"\n",
        "    },\n",
        "    \"Our car rental service provides insurance coverage and a fuel-efficient vehicle for your trip.\": {\n",
        "        \"Service\": \"Car rental\",\n",
        "        \"Insurance\": \"Coverage included\",\n",
        "        \"Vehicle\": \"Fuel-efficient\"\n",
        "    },\n",
        "    \"The mortgage rate for this property is 3.5% with a 30-year term.\": {\n",
        "        \"Property\": \"Real estate property\",\n",
        "        \"Interest Rate\": \"3.5%\",\n",
        "        \"Loan Term\": \"30 years\"\n",
        "    },\n",
        "    \"Our fitness membership includes access to all gym facilities, group classes, and personal training sessions.\": {\n",
        "        \"Membership\": \"Fitness membership\",\n",
        "        \"Access\": \"Gym facilities, group classes, personal training\"\n",
        "    },\n",
        "    \"The credit card offers a cashback reward of 2% on all purchases and no annual fee.\": {\n",
        "        \"Credit Card\": \"Credit card\",\n",
        "        \"Rewards\": \"2% cashback\",\n",
        "        \"Fees\": \"No annual fee\"\n",
        "    },\n",
        "    \"The new Samsung Galaxy S22 Ultra, with its 108MP camera, is available for pre-order at $1,199, and you can trade in your old device for an extra $300 off.\": {\n",
        "        \"Product\": \"Samsung Galaxy S22 Ultra\",\n",
        "        \"Camera\": \"108MP\",\n",
        "        \"Price\": \"$1,199\",\n",
        "        \"Discount\": \"$300 off (with trade-in)\"\n",
        "    },\n",
        "    \"Our 65-inch 8K OLED TV, featuring Dolby Atmos sound and smart home integration, is on sale for $1,999, down from the original price of $2,499.\": {\n",
        "        \"Product\": \"65-inch 8K OLED TV\",\n",
        "        \"Features\": \"Dolby Atmos sound, smart home integration\",\n",
        "        \"Original Price\": \"$2,499\",\n",
        "        \"Sale Price\": \"$1,999\"\n",
        "    },\n",
        "    \"Experience a luxury cruise with our 10-day Mediterranean package on the Grand Oceanic cruise liner, inclusive of all meals and excursions, starting at $3,499 per person.\": {\n",
        "        \"Package\": \"10-day Mediterranean cruise\",\n",
        "        \"Cruise Liner\": \"Grand Oceanic\",\n",
        "        \"Inclusions\": \"All meals, excursions\",\n",
        "        \"Price\": \"$3,499 per person\"\n",
        "    },\n",
        "    \"Upgrade your home office setup with the latest MacBook Pro, equipped with an M2 chip, 16GB RAM, and a 512GB SSD, available for $1,799.\": {\n",
        "        \"Product\": \"MacBook Pro\",\n",
        "        \"Processor\": \"M2 chip\",\n",
        "        \"RAM\": \"16GB\",\n",
        "        \"Storage\": \"512GB SSD\",\n",
        "        \"Price\": \"$1,799\"\n",
        "    },\n",
        "    \"Join our premium gym membership program, which includes access to personal trainers, nutrition coaching, and unlimited classes for $99 per month, with a 20% discount on the first three months.\": {\n",
        "        \"Membership\": \"Premium gym membership\",\n",
        "        \"Inclusions\": \"Personal trainers, nutrition coaching, unlimited classes\",\n",
        "        \"Monthly Price\": \"$99\",\n",
        "        \"Discount\": \"20% off (first three months)\",\n",
        "        \"time period\" : \"3 Months\"\n",
        "    },\n",
        "    \"Get a home loan with a fixed interest rate of 3.25% for a 15-year term, and we offer a $5,000 credit towards closing costs for qualified applicants.\": {\n",
        "        \"Loan Type\": \"Home loan\",\n",
        "        \"Interest Rate\": \"3.25% (fixed)\",\n",
        "        \"Loan Term\": \"15 years\",\n",
        "        \"Closing Costs\": \"$5,000 credit (qualified applicants)\"\n",
        "    },\n",
        "    \"The annual membership for our streaming service provides 4K streaming, offline downloads, and access to exclusive content for $12.99 per month, billed annually.\": {\n",
        "        \"Service\": \"Streaming service\",\n",
        "        \"Features\": \"4K streaming, offline downloads, exclusive content\",\n",
        "        \"Monthly Price\": \"$12.99\",\n",
        "        \"Billing\": \"Annually\"\n",
        "    },\n",
        "    \"Fly with our airline and enjoy complimentary in-flight meals, a 30kg baggage allowance, and free Wi-Fi on all international flights.\": {\n",
        "        \"Airline\": \"Our airline\",\n",
        "        \"Inclusions\": \"Complimentary meals, 30kg baggage allowance, free Wi-Fi (international flights)\"\n",
        "    },\n",
        "    \"Our credit card offers a 1.5% cashback reward on all purchases, no foreign transaction fees, and a $200 bonus for new cardholders who spend $1,500 in the first 3 months.\": {\n",
        "        \"Credit Card\": \"Credit card\",\n",
        "        \"Rewards\": \"1.5% cashback\",\n",
        "        \"Fees\": \"No foreign transaction fees\",\n",
        "        \"Bonus\": \"$200 (for new cardholders spending $1,500 in the first 3 months)\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd9P95wX_OQK",
        "outputId": "cf0ec699-f0db-4376-c008-53e1e411d0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch<2.2.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2.0,>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2.0,>=1.10.0->accelerate) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K9Atfl0ekBS"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import TensorDataset\n",
        "\n",
        "# # Tokenize the input texts and labels\n",
        "# inputs = tokenizer(list((training_data.keys())), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "# labels = tokenizer([\" \".join([f\"{field}: {value}\" for field, value in fields.items()]) for fields in training_data.values()], padding=True, truncation=True, return_tensors=\"pt\", max_length=128)[\"input_ids\"]\n",
        "\n",
        "# # Create a PyTorch dataset\n",
        "# dataset = TensorDataset((inputs[\"input_ids\"]), inputs[\"attention_mask\"], labels)\n",
        "\n",
        "# # Initialize the data collator\n",
        "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./fine_tuned\",\n",
        "#     num_train_epochs=3,  # Adjust as needed\n",
        "#     per_device_train_batch_size=8,  # Adjust as needed\n",
        "#     save_steps=500,\n",
        "#     save_total_limit=2,\n",
        "# )\n",
        "\n",
        "# # Initialize Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset,\n",
        "#     data_collator=data_collator,\n",
        "# )\n",
        "\n",
        "# # Start fine-tuning\n",
        "# trainer.train()\n",
        "\n",
        "# # Save the fine-tuned model and tokenizer\n",
        "# model.save_pretrained(\"./fine_tuned\")\n",
        "# tokenizer.save_pretrained(\"./fine_tuned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ZA6NICmr0t"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U225Gq_klXbd"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# print(\"Using device:\", device)\n",
        "\n",
        "# MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME,\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-8PwSE8jep"
      },
      "outputs": [],
      "source": [
        "# from transformers import DataCollatorWithPadding\n",
        "\n",
        "# # Initialize lists to store tokenized inputs and labels\n",
        "# input_texts = []\n",
        "# labels = []\n",
        "\n",
        "# # Tokenize the data and create labels\n",
        "# for text, fields in training_data.items():\n",
        "#     input_texts.append(text)\n",
        "#     label = \" \".join([f\"{field}: {value}\" for field, value in fields.items()])\n",
        "#     labels.append(label)\n",
        "\n",
        "# tokenizer.pad = tokenizer.pad\n",
        "\n",
        "# # Tokenize the input texts\n",
        "# tokenized_inputs = tokenizer(\n",
        "#     input_texts,\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128  # Adjust the max length as needed\n",
        "# )\n",
        "\n",
        "# # Convert labels to tensors\n",
        "# label_ids = tokenizer(\n",
        "#     labels,\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128  # Adjust the max length as needed\n",
        "# )[\"input_ids\"]\n",
        "\n",
        "\n",
        "# # Create a PyTorch dataset\n",
        "# dataset = torch.utils.data.TensorDataset(\n",
        "#     tokenized_inputs[\"input_ids\"],\n",
        "#     tokenized_inputs[\"attention_mask\"],\n",
        "#     label_ids\n",
        "# )\n",
        "\n",
        "# # Calculate the total number of training steps\n",
        "# num_train_epochs = 3\n",
        "# batch_size = 8  # Adjust batch size as needed\n",
        "# total_steps = (len(dataset) // batch_size) * num_train_epochs\n",
        "\n",
        "# data_collator = DataCollatorWithPadding(dataset , pad_to_multiple_of=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-jghC7MrFV"
      },
      "outputs": [],
      "source": [
        "# data_collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LzlBv5S4w31"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "# import transformers\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./phi2_fine_tuned\",\n",
        "#     per_device_train_batch_size=batch_size,  # Adjust batch size as needed\n",
        "#     num_train_epochs=num_train_epochs,             # Adjust the number of epochs\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=500,                 # Evaluate the model every 500 steps\n",
        "#     save_steps=500,                 # Save checkpoints every 500 steps\n",
        "#     save_total_limit=2,             # Limit the number of checkpoints to save\n",
        "#     max_steps = total_steps\n",
        "# )\n",
        "\n",
        "# # Define a Trainer for fine-tuning\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     data_collator = data_collator,\n",
        "#     train_dataset = dataset\n",
        "# )\n",
        "\n",
        "# # Start fine-tuning\n",
        "# trainer.train()\n",
        "\n",
        "# # Save the fine-tuned model\n",
        "# model.save_pretrained(\"./phi2_fine_tuned\")\n",
        "\n",
        "# # Optionally, save the tokenizer as well\n",
        "# tokenizer.save_pretrained(\"./phi2_fine_tuned_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh4Q0slG5XqL"
      },
      "outputs": [],
      "source": [
        "# tokenized_inputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39LLa_oLYMmh"
      },
      "outputs": [],
      "source": [
        "# data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNO2AiwabuQI"
      },
      "outputs": [],
      "source": [
        "def convert_to_training_format(training_data):\n",
        "  formatted_text = []\n",
        "  for text,labels in training_data.items():\n",
        "    input_text = text\n",
        "    target_text = labels\n",
        "    formatted_data.append((input_text, target_text))\n",
        "  return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pu0mhYjrL9",
        "outputId": "7d3263f0-06b0-4eca-9d5a-ee61fde20253"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The iPhone 13 Pro is priced at $999, but we have a limited-time offer with a 20% discount.',\n",
              "  {'Product': 'iPhone 13 Pro', 'Price': '$999', 'Discount': '20%'}),\n",
              " ('The Galaxy Watch 4 comes with a 2-year warranty and free shipping.',\n",
              "  {'Product': 'Galaxy Watch 4', 'Warranty': '2 years', 'Shipping': 'Free'}),\n",
              " ('Our new smart TV model features a 55-inch 4K OLED display and built-in streaming apps.',\n",
              "  {'Product': 'Smart TV',\n",
              "   'Display': '55-inch 4K OLED',\n",
              "   'Features': 'Built-in streaming apps'}),\n",
              " ('This laptop has an Intel Core i7 processor, 16GB RAM, and a 512GB SSD for fast performance.',\n",
              "  {'Product': 'Laptop',\n",
              "   'Processor': 'Intel Core i7',\n",
              "   'RAM': '16GB',\n",
              "   'Storage': '512GB SSD'}),\n",
              " ('Our vacation package includes a 7-day stay at a 5-star beachfront resort and daily breakfast.',\n",
              "  {'Package': 'Vacation package',\n",
              "   'Duration': '7 days',\n",
              "   'Accommodation': '5-star beachfront resort',\n",
              "   'Inclusions': 'Daily breakfast'}),\n",
              " ('The annual subscription to our streaming service offers unlimited access to a vast library of movies and TV shows.',\n",
              "  {'Service': 'Streaming service',\n",
              "   'Subscription': 'Annual',\n",
              "   'Content': 'Movies and TV shows'}),\n",
              " ('Our car rental service provides insurance coverage and a fuel-efficient vehicle for your trip.',\n",
              "  {'Service': 'Car rental',\n",
              "   'Insurance': 'Coverage included',\n",
              "   'Vehicle': 'Fuel-efficient'}),\n",
              " ('The mortgage rate for this property is 3.5% with a 30-year term.',\n",
              "  {'Property': 'Real estate property',\n",
              "   'Interest Rate': '3.5%',\n",
              "   'Loan Term': '30 years'}),\n",
              " ('Our fitness membership includes access to all gym facilities, group classes, and personal training sessions.',\n",
              "  {'Membership': 'Fitness membership',\n",
              "   'Access': 'Gym facilities, group classes, personal training'}),\n",
              " ('The credit card offers a cashback reward of 2% on all purchases and no annual fee.',\n",
              "  {'Credit Card': 'Credit card',\n",
              "   'Rewards': '2% cashback',\n",
              "   'Fees': 'No annual fee'}),\n",
              " ('The new Samsung Galaxy S22 Ultra, with its 108MP camera, is available for pre-order at $1,199, and you can trade in your old device for an extra $300 off.',\n",
              "  {'Product': 'Samsung Galaxy S22 Ultra',\n",
              "   'Camera': '108MP',\n",
              "   'Price': '$1,199',\n",
              "   'Discount': '$300 off (with trade-in)'}),\n",
              " ('Our 65-inch 8K OLED TV, featuring Dolby Atmos sound and smart home integration, is on sale for $1,999, down from the original price of $2,499.',\n",
              "  {'Product': '65-inch 8K OLED TV',\n",
              "   'Features': 'Dolby Atmos sound, smart home integration',\n",
              "   'Original Price': '$2,499',\n",
              "   'Sale Price': '$1,999'}),\n",
              " ('Experience a luxury cruise with our 10-day Mediterranean package on the Grand Oceanic cruise liner, inclusive of all meals and excursions, starting at $3,499 per person.',\n",
              "  {'Package': '10-day Mediterranean cruise',\n",
              "   'Cruise Liner': 'Grand Oceanic',\n",
              "   'Inclusions': 'All meals, excursions',\n",
              "   'Price': '$3,499 per person'}),\n",
              " ('Upgrade your home office setup with the latest MacBook Pro, equipped with an M2 chip, 16GB RAM, and a 512GB SSD, available for $1,799.',\n",
              "  {'Product': 'MacBook Pro',\n",
              "   'Processor': 'M2 chip',\n",
              "   'RAM': '16GB',\n",
              "   'Storage': '512GB SSD',\n",
              "   'Price': '$1,799'}),\n",
              " ('Join our premium gym membership program, which includes access to personal trainers, nutrition coaching, and unlimited classes for $99 per month, with a 20% discount on the first three months.',\n",
              "  {'Membership': 'Premium gym membership',\n",
              "   'Inclusions': 'Personal trainers, nutrition coaching, unlimited classes',\n",
              "   'Monthly Price': '$99',\n",
              "   'Discount': '20% off (first three months)',\n",
              "   'time period': '3 Months'}),\n",
              " ('Get a home loan with a fixed interest rate of 3.25% for a 15-year term, and we offer a $5,000 credit towards closing costs for qualified applicants.',\n",
              "  {'Loan Type': 'Home loan',\n",
              "   'Interest Rate': '3.25% (fixed)',\n",
              "   'Loan Term': '15 years',\n",
              "   'Closing Costs': '$5,000 credit (qualified applicants)'}),\n",
              " ('The annual membership for our streaming service provides 4K streaming, offline downloads, and access to exclusive content for $12.99 per month, billed annually.',\n",
              "  {'Service': 'Streaming service',\n",
              "   'Features': '4K streaming, offline downloads, exclusive content',\n",
              "   'Monthly Price': '$12.99',\n",
              "   'Billing': 'Annually'}),\n",
              " ('Fly with our airline and enjoy complimentary in-flight meals, a 30kg baggage allowance, and free Wi-Fi on all international flights.',\n",
              "  {'Airline': 'Our airline',\n",
              "   'Inclusions': 'Complimentary meals, 30kg baggage allowance, free Wi-Fi (international flights)'}),\n",
              " ('Our credit card offers a 1.5% cashback reward on all purchases, no foreign transaction fees, and a $200 bonus for new cardholders who spend $1,500 in the first 3 months.',\n",
              "  {'Credit Card': 'Credit card',\n",
              "   'Rewards': '1.5% cashback',\n",
              "   'Fees': 'No foreign transaction fees',\n",
              "   'Bonus': '$200 (for new cardholders spending $1,500 in the first 3 months)'})]"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_data = []\n",
        "formatted_data = convert_to_training_format(training_data)\n",
        "formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0gBflwyjtSP"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def _init_(self, tokenizer, texts, targets, max_length=512):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.texts = texts\n",
        "    self.targets = targets\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def _len_(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def _getitem_(self, idx):\n",
        "    input_text = str(self.texts[idx])\n",
        "    target_text = str(self.targets[idx])\n",
        "\n",
        "    input_encoding = tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True)\n",
        "    target_encoding = tokenizer(target_text, max_length=self.max_length, padding='max_length', truncation=True)\n",
        "\n",
        "    inputs = {key: torch.tensor(val).to(device) for key, val in input_encoding.items()}\n",
        "    targets = {key: torch.tensor(val).to(device) for key, val in target_encoding.items()}\n",
        "\n",
        "    return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EfAslnrcjy2t",
        "outputId": "d99f523a-5db9-4020-fd7a-a70a7a2903b2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "CustomDataset() takes no arguments",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-168303f44276>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mformatted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CustomDataset() takes no arguments"
          ]
        }
      ],
      "source": [
        "texts, targets = zip(*formatted_data)\n",
        "dataset = CustomDataset(tokenizer = tokenizer, texts = texts, targets = targets)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhV6WZhkj4Rj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
